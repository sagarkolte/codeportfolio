{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "I trained my LSTM using the very clear and concise example in this notebook(By Peter Nagy):\n",
    "\n",
    "- https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras\n",
    "\n",
    "I learned how to use Word2Vec embeddings in my LSTM using the instructions in this notebook(user:lystdo):\n",
    "\n",
    "- https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings\n",
    "\n",
    "My First course on using word embeddings in python came from here:\n",
    "\n",
    "- https://ai.intelligentonlinetools.com/ml/text-vectors-word-embeddings-word2vec/\n",
    "\n",
    "A great trivial example of a LSTM implemetation, get the important stuff straight:\n",
    "\n",
    "https://www.youtube.com/watch?v=iMIWee_PXl8\n",
    "\n",
    "Adding an embedding layer to your LSTM in Keras\n",
    "\n",
    "https://www.youtube.com/watch?v=8h8Z_pKyifM\n",
    "\n",
    "## Reference material for Transformers and BERT\n",
    "\n",
    "This is a great lecture by Pascal Pupoart as a first introduction to Transformers and Multi Headed Attention.\n",
    "\n",
    "- https://youtu.be/OyFJWRnt_AY\n",
    "\n",
    "This article by Jay Alammar is a great step by step documentation of the vector transformations that happen in a transformer.\n",
    "\n",
    "- http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "This is a tutorial on using BERT as a classifier in the NLP context.\n",
    "\n",
    "https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.options.display.max_colwidth = 2000\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import math\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "#---------------------------------------->Sentiment Analyser Libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "#---------------------------------------->Sentiment Analyser Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install\n",
    "!pip install pytorch-pretrained-bert pytorch-nlp\n",
    "\n",
    "# BERT imports\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#% matplotlib inline\n",
    "\n",
    "# specify GPU device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\n",
    "# Keeping only the neccessary columns\n",
    "data1 = df_train[['text','sentiment']]\n",
    "data1['n_sentiment'] = data1.apply(lambda row: 'p' if row['sentiment']=='positive' else ('n' if row['sentiment']=='negative' else 'neu'), axis = 1)#--->Making the sentiment numeric\n",
    "data1 = data1[['text','n_sentiment']]\n",
    "data1.columns=['text','sentiment']\n",
    "data2 = df_train[['selected_text','sentiment']]\n",
    "data2.columns=['text','sentiment']\n",
    "data2['n_sentiment'] = data2.apply(lambda row: 'vp' if row['sentiment']=='positive' else ('vn' if row['sentiment']=='negative' else 'neu'), axis = 1)#--->Making the sentiment numeric\n",
    "data2 = data2[['text','n_sentiment']]\n",
    "data2.columns=['text','sentiment']\n",
    "data =  pd.concat([data1,data2])#-------------------------------------------------------------------------->Final Training Data\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',str(x))))#------------------------->Cleaning the tweets\n",
    "print(len(data))\n",
    "data['text_len'] = data.apply(lambda row:len(row['text']),axis=1)\n",
    "data = data[data['text_len']!=0]\n",
    "print(len(data))\n",
    "\n",
    "#Tokenize the data\n",
    "max_fatures = 2000\n",
    "#tokenizer = Tokenizer(num_words=max_fatures,split=' ')\n",
    "tokenizer = Tokenizer(split=' ')\n",
    "tokenizer = Tokenizer(split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "print(len(X[2]))\n",
    "X = pad_sequences(X)\n",
    "print(len(X[2]))\n",
    "\n",
    "#Split into training and test\n",
    "Y = pd.get_dummies(data['sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "#print(X_train.shape,Y_train.shape)\n",
    "#print(X_test.shape,Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the data for Bert\n",
    "\n",
    "text_bert = list(data['text'])#-------------------->List of sentences\n",
    "senti_bert = list(data['sentiment'])#-------------->List of Labels\n",
    "senti_dict = {'neu':0,'p':1,'vp':2,'n':3,'vn':4}#-->Dictionary to convert labels to integers\n",
    "f = lambda st:senti_dict[st]#---------------------->Function to assign label to the integer\n",
    "senti_bert = [f(t) for t in senti_bert]#----------->Converting labels to integers\n",
    "# add special tokens for BERT to work properly\n",
    "text_bert = [\"[CLS] \" + t + \" [SEP]\" for t in text_bert]#-->Preparing sentences to be consumed by BERT\n",
    "\n",
    "\n",
    "# Tokenize with BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)#---->Creating a Tokenizer\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in text_bert]#-------------------->Tokenizing the sentences\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. \n",
    "MAX_LEN = 128#----------------------------->Is this the Max length of the sentences in words?\n",
    "# Pad our input tokens\n",
    "#Each word is converted into an integer and padding is applied\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")#------->Each word is converted into an integer and padding is applied\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "#This is just a repeat of the above code.\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []#----------------------------------------->We need to understand what attention masks do.\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]#----------------------->Creates a list of True/False values True everywhere except for the padding\n",
    "  attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "#This is self explanatory\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, senti_bert, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "                                             \n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)#------------------------------------------------->What are torch tensors?\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "# Select a batch size for training. \n",
    "batch_size = 32#---------------------------------------------------------------------------->Need to understand what batch size is.\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader#------------------------------------->What is an iterator here? \n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)#------------------------>What does TensorDataset do?\n",
    "train_sampler = RandomSampler(train_data)#-------------------------------------------------->What does RandomSampler do?\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)#---->What is a DataLoader?\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)#------------------------------------>What is a SequentialSampler?\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=5)#--->Declaring the Model\n",
    "model.cuda()#------------------------------------------------------------------------------>What does .cuda() do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BERT fine-tuning parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "  \n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "# Number of training epochs \n",
    "epochs = 4\n",
    "\n",
    "# BERT training loop\n",
    "for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "  \n",
    "  ## TRAINING\n",
    "  \n",
    "  # Set our model to training mode\n",
    "  model.train()  \n",
    "  # Tracking variables\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    train_loss_set.append(loss.item())    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "      \n",
    "  ## VALIDATION\n",
    "\n",
    "  # Put model in evaluation mode\n",
    "  model.eval()\n",
    "  # Tracking variables \n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "\n",
    "# plot training performance\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_set)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_bert = list(data['text'])#-------------------->List of sentences\n",
    "senti_bert = list(data['sentiment'])#-------------->List of Labels\n",
    "senti_dict = {'neu':0,'p':1,'vp':2,'n':3,'vn':4}#-->Dictionary to convert labels to integers\n",
    "f = lambda st:senti_dict[st]#---------------------->Function to assign label to the integer\n",
    "senti_bert = [f(t) for t in senti_bert]#----------->Converting labels to integers\n",
    "\n",
    "\n",
    "# load test data\n",
    "sentences = [\"[CLS] \" + query + \" [SEP]\" for query in text_bert]\n",
    "labels = senti_bert\n",
    "\n",
    "# tokenize test data\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "MAX_LEN = 128\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask) \n",
    "\n",
    "# create test tensors\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "batch_size = 32  \n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "## Prediction on test set\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "  with torch.no_grad():\n",
    "    # Forward pass, calculate logit predictions\n",
    "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "  \n",
    "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "matthews_set = []\n",
    "for i in range(len(true_labels)):\n",
    "  matthews = matthews_corrcoef(true_labels[i],\n",
    "                 np.argmax(predictions[i], axis=1).flatten())\n",
    "  matthews_set.append(matthews)\n",
    "  \n",
    "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "print('Classification accuracy using BERT Fine Tuning: {0:0.2%}'.format(matthews_corrcoef(flat_true_labels, flat_predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in prediction_dataloader:\n",
    "    print(len(batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peparing the Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_word(word_index,i):\n",
    "    index_word_items=word_index.items()\n",
    "    j=[item for item in index_word_items if item[1]==i]\n",
    "    return j[0][0]\n",
    "\n",
    "def get_emb_mat(data,l_input_vec):\n",
    "    tokenizer = Tokenizer(split=' ')\n",
    "    tokenizer.fit_on_texts(data['text'].values)\n",
    "    word_index = tokenizer.word_index#---------------------------------------->Needed for embedding matrix\n",
    "    sentences = [word_tokenize(text) for text in list(data['text'])]\n",
    "    #Get a list of tokenized sentences\n",
    "    w2vmodel = Word2Vec(sentences, min_count=1, size=l_input_vec)#------------->Needs tokenized sentences\n",
    "    vocab = w2vmodel.wv.vocab.keys()\n",
    "    vocab_dict = w2vmodel.wv.vocab\n",
    "    wordsInVocab = len(vocab)\n",
    "\n",
    "    nb_words = len(word_index)+1\n",
    "\n",
    "    embedding_matrix = np.zeros((nb_words, l_input_vec))#----------------------->Needs word_index and Word2Vec model\n",
    "    for word, i in word_index.items():\n",
    "        if word in vocab:\n",
    "            embedding_matrix[i] = w2vmodel.wv.word_vec(word)\n",
    "    print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    return embedding_matrix,vocab_dict,w2vmodel\n",
    "\n",
    "def train_nn(model,X_train,Y_train,batch_size,epochs):\n",
    "    #batch_size = 34\n",
    "    #batch_size = 20\n",
    "    model.fit(X_train, Y_train, epochs = 12, batch_size=batch_size, verbose = 2)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_emb_word_vectors(word_index,model,l_input_vec,i):\n",
    "    try:\n",
    "        word = int_to_word(word_index,i)\n",
    "        vec = model[word]\n",
    "        return vec\n",
    "    except:\n",
    "        return [0 for i in range(l_input_vec)]\n",
    "\n",
    "#w2vmodel = get_emb_mat(data,100)[2]\n",
    "#get_emb_word_vectors(word_index,w2vmodel,100,1)\n",
    "\n",
    "def get_emb_sentence_vectors(word_index,model,l_input_vec,X):\n",
    "    g = lambda l:[get_emb_word_vectors(word_index,model,l_input_vec,i) for i in l]\n",
    "    X_emb = [g(k) for k in X]\n",
    "    return X_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_input_vec = 100\n",
    "lstm_out = 196\n",
    "batch_size = 20\n",
    "epochs = 12\n",
    "spatial_drp = 0.4\n",
    "drp = 0.2\n",
    "rec_drp = 0.2\n",
    "dense_out = np.shape(Y)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring the LSTM Model and training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = get_emb_mat(data,l_input_vec)[0]\n",
    "embed_dim = l_input_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(embedding_matrix), embed_dim ,weights=[embedding_matrix],input_length = 34,trainable=False))\n",
    "model.add(SpatialDropout1D(spatial_drp))\n",
    "model.add(LSTM(lstm_out, dropout=drp, recurrent_dropout=rec_drp))\n",
    "model.add(Dense(dense_out,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "train_nn(model,X_train,Y_train,batch_size,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def sortSecond(val): \n",
    "    return val[1] \n",
    "\n",
    "def get_ngrams(doc,n):\n",
    "    tokens = doc.split()\n",
    "    all_ngrams = ngrams(tokens, n)\n",
    "    return all_ngrams\n",
    "\n",
    "def get_everygrams(doc):\n",
    "    tokens = doc.split()\n",
    "    every_grams = list(everygrams(tokens))\n",
    "    every_grams = [' '.join(k) for k in every_grams]\n",
    "    return every_grams\n",
    "\n",
    "\n",
    "def lstm_sent(text,sent):\n",
    "    twt = [text]\n",
    "    #vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "    twt = tokenizer.texts_to_sequences(twt)\n",
    "    #padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "    twt = pad_sequences(twt, maxlen=34, dtype='int32', value=0)\n",
    "    #print(twt)\n",
    "    sentiment_n = model.predict(twt,batch_size=1,verbose = 2)\n",
    "    #sentiment = translate(flatten(list(sentiment_n[0])))\n",
    "    sentiment = translate(list(sentiment_n[0]),sent)\n",
    "    return sentiment,sentiment_n\n",
    "\n",
    "    \n",
    "def vader_sent(sentence,a):\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    if sentiment_dict['compound'] >= a :\n",
    "        return \"positive\" \n",
    "    elif sentiment_dict['compound'] <= a*(-1) :\n",
    "        return \"negative\"\n",
    "    else :\n",
    "        return \"neutral\"\n",
    "    \n",
    "def vader_sent_num(sentence):\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    sent_num = sentiment_dict['compound']\n",
    "    return sent_num\n",
    "\n",
    "def get_sig_ngram(sentence,sentiment,perc):\n",
    "    n = int(math.ceil(len(sentence.split())*perc))\n",
    "    g = lambda sentiment:max if sentiment =='positive' else min\n",
    "    #sentence = custom_spell(str(sentence))\n",
    "    ngrams = get_ngrams(sentence,n)\n",
    "    sent_dict = [(' '.join(k),vader_sent_num(' '.join(k))) for k in ngrams]\n",
    "    prominent_sent = g(sentiment)([v[1] for v in sent_dict])\n",
    "    prom_ngrams = [ntup for ntup in sent_dict if ntup[1]==prominent_sent]\n",
    "    return prom_ngrams[0][0]\n",
    "\n",
    "def get_sig_ngram_inv(sentence,sentiment,perc):\n",
    "    n = int(math.ceil(len(sentence.split())*perc))\n",
    "    g = lambda sentiment:max if sentiment =='positive' else min\n",
    "    #sentence = custom_spell(str(sentence))\n",
    "    ngrams = get_ngrams(sentence,n)\n",
    "    sent_dict = [(' '.join(k),vader_sent_num(' '.join(k))) for k in ngrams]\n",
    "    prominent_sent = g(sentiment)([v[1] for v in sent_dict])\n",
    "    prom_ngrams = [ntup for ntup in sent_dict if ntup[1]==prominent_sent]\n",
    "    return sent_dict\n",
    "\n",
    "\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions translate the ratings from vectors to the original categories\n",
    "def flatten(l):\n",
    "    m = max(l)\n",
    "    g = lambda i,m: 1 if i>=m else 0\n",
    "    l = list(map(g,l,[m]*len(l)))\n",
    "    return l\n",
    "def translate(l,sent):\n",
    "    s_categories = ['n','neu','p','vn','vp']\n",
    "    g = lambda l: l[s_categories.index(sent)]\n",
    "    #sentiment = s_categories[g(l)]\n",
    "    sentiment = g(l)\n",
    "    return sentiment\n",
    "\n",
    "def get_jaccard(df,rows):\n",
    "    \n",
    "    df1=df.sample(rows)\n",
    "    #print(df1)\n",
    "    g = lambda senti:'vp' if senti=='positive' else 'vn'\n",
    "    df1['algo_text'] = df1.apply(lambda row:str(row['text']) if row['sentiment']=='neutral' else get_f_text(str(row['text']),g(row['sentiment'])),axis=1)\n",
    "    df1['jaccard'] = df1.apply(lambda row:jaccard(str(row['selected_text']),str(row['algo_text'])),axis=1)\n",
    "    score = np.average(list(df1['jaccard']))\n",
    "    return score,df1\n",
    "def get_jaccard_thresh(df,rows,perc,thresh):\n",
    "    df1=df.sample(rows)\n",
    "    \n",
    "    df1['algo_text'] = df1.apply(lambda row:row['text'] if row['sentiment']=='neutral' else get_sig_ngram(row['text'],row['sentiment'],perc),axis=1)\n",
    "    df1['jaccard'] = df1.apply(lambda row:jaccard(str(row['selected_text']),str(row['algo_text'])),axis=1)\n",
    "    score = np.average(list(df1['jaccard']))\n",
    "    df2 = df1[df1['jaccard']<thresh] \n",
    "    return score,df2\n",
    "\n",
    "def get_sel_text(text,sent):\n",
    "    f = lambda x: re.sub('[^a-zA-z0-9\\s]',' ',x)\n",
    "    #g = lambda x: str(x).lower()\n",
    "    #text = g(text)\n",
    "    text = f(text)\n",
    "    e = get_everygrams(text)\n",
    "    lis = [(i,lstm_sent(i,sent)[0]) for i in e]\n",
    "    #lis = [i for i in lis if i[1]==sent]\n",
    "    return lis\n",
    "\n",
    "def get_f_text(text,sent):\n",
    "    t_list = get_sel_text(text,sent)\n",
    "    m = max([t[1] for t in t_list])\n",
    "    f = [t for t in t_list if t[1]>=m]\n",
    "    return f[0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score,df1 = get_jaccard(df_train,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[df1['sentiment']!='neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sort_values('jaccard').tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
