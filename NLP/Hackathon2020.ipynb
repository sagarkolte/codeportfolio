{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import shutil\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import datetime\n",
    "import itertools\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from sklearn.cluster import AffinityPropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a list of phrases to match\n",
    "person_path = 'resources/person_list.txt'\n",
    "persons = []\n",
    "\n",
    "with open(person_path, 'r') as filehandle:\n",
    "    persons = [current_place.rstrip() for current_place in filehandle.readlines()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "hnwi_patterns = list(nlp.pipe(persons))\n",
    "matcher_hnwi = PhraseMatcher(nlp.vocab)\n",
    "matcher_hnwi.add('HNWI', None, *hnwi_patterns)\n",
    "\n",
    "\n",
    "def overlap_detect(s_1,e_1,start,end):\n",
    "    return (start<=s_1<=end) or (start<=e_1<=end) or (s_1<=start<=end<=e_1)\n",
    "\n",
    "def span_overlap_detect(span1,span2):\n",
    "    s_1 = span1.start_char\n",
    "    e_1 = span1.end_char\n",
    "    start = span2.start_char\n",
    "    end = span2.end_char\n",
    "    return overlap_detect(s_1,e_1,start,end)\n",
    "\n",
    "def span_discard(span, span_list):\n",
    "    filtered_list = [s for s in span_list if not span_overlap_detect(span,s)]\n",
    "    return filtered_list\n",
    "\n",
    "def span_discard_list(s_list1, s_list2):\n",
    "    filtered_list2 = [span_discard(s,s_list2) for s in s_list1]\n",
    "    merged = list(itertools.chain.from_iterable(filtered_list2))\n",
    "    merged = list(set(merged))\n",
    "    return merged\n",
    "\n",
    "def hnwi_component(doc):\n",
    "    matches = matcher_hnwi(doc)\n",
    "    orig_ents = doc.ents\n",
    "    #print(orig_ents)\n",
    "    spans = [Span(doc,start,end,label='HNWI') for match_id, start, end in matches]\n",
    "    \n",
    "    try:\n",
    "        doc.ents = span_discard_list(spans,orig_ents)+spans\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        doc.ents = orig_ents\n",
    "    \n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(hnwi_component, before = 'ner')\n",
    "\n",
    "def get_key_list(d,val):\n",
    "    items = d.items()\n",
    "    rel_items = [item for item in items if item[1]==val]\n",
    "    key_list = [item[0] for item in rel_items]\n",
    "    r_d = {val:key_list}\n",
    "    return r_d\n",
    "\n",
    "def dict_to_df(d):\n",
    "    vals = list(set(d.values()))\n",
    "    dict_list = [get_key_list(d,val) for val in vals]\n",
    "    dict_f = {list(di.keys())[0]:[','.join(list(di.values())[0])] for di in dict_list}\n",
    "    df = pd.DataFrame(dict_f)\n",
    "    return df\n",
    "\n",
    "def text_to_df(text):\n",
    "    doc = nlp(text)\n",
    "    d = dict([(ent.text,ent.label_) for ent in doc.ents])\n",
    "    df = dict_to_df(d)\n",
    "    #print(text,df)\n",
    "    return df\n",
    "\n",
    "def get_vector(text):\n",
    "    doc = nlp(text)\n",
    "    vec = np.array(doc.vector)\n",
    "    return vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_list = 'resources/rss_feed_list.txt'\n",
    "dataset = 'out/out.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_scores(sentence):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    score = analyzer.polarity_scores(sentence)\n",
    "    return score['compound']\n",
    "\n",
    "def url_to_df(src_url):\n",
    "    resp = requests.get(src_url[1])\n",
    "    soup = BeautifulSoup(resp.content, features = 'xml')\n",
    "    items = soup.findAll('item')\n",
    "    news_items = []\n",
    "    for item in items:\n",
    "        news_item = {}\n",
    "        news_item['Title'] = item.title.text.replace(\"'\",\"\")\n",
    "        #print(text_to_df(item.title.text.replace(\"'\",\"\")))\n",
    "        news_item['Description'] = item.description.text\n",
    "        news_item['PubDate'] = item.pubDate.text\n",
    "        news_item['Source'] = src_url[0]\n",
    "        news_item['Title_Sentiment'] = sentiment_analyzer_scores(item.title.text)\n",
    "        news_item['Description_Sentiment'] = sentiment_analyzer_scores(item.description.text)\n",
    "        news_item['vector'] = get_vector(item.title.text)\n",
    "        #news_item['bsScore'] = bsfilter(item.title.text)\n",
    "        news_items.append(news_item)\n",
    "    df = pd.DataFrame(news_items)\n",
    "    return df\n",
    "\n",
    "def create_nlp_cols(df,col_name):\n",
    "    df_list = [pd.DataFrame(df[i:i+1]).reset_index(drop=True) for i in range(len(df))]\n",
    "    df_list = [pd.concat([df_list[i],text_to_df(df_list[i][col_name][0])],axis=1) for i in range(len(df))]\n",
    "    df_final = pd.concat(df_list,axis=0,sort=False).reset_index(drop=True)\n",
    "    return df_final\n",
    "\n",
    "def get_daily_df():\n",
    "    # define empty list\n",
    "    urls = []\n",
    "    sources = []\n",
    "    # open file and read the content in a list\n",
    "    with open(feed_list, 'r') as filehandle:\n",
    "        urls = [url.rstrip().split('|')[1] for url in filehandle.readlines()]\n",
    "    with open(feed_list, 'r') as filehandle:\n",
    "        sources = [url.rstrip().split('|')[0] for url in filehandle.readlines()]\n",
    "    src_url_pairs = list(zip(sources,urls))\n",
    "    df_list = [url_to_df(src_url) for src_url in src_url_pairs]\n",
    "    \n",
    "    #df_list = [pd.concat([df,text_to_df(df['Title'][0])],axis=1) for df in df_list]\n",
    "    df_final = pd.concat(df_list,axis=0).reset_index(drop=True)\n",
    "    #df_final = df_final[df_final['bsScore']>=0.5]\n",
    "    df_final = create_nlp_cols(df_final,'Title')\n",
    "    names = list(set(list(df_final['HNWI'].dropna())))\n",
    "    df_list = [create_time_line(df_final,name) for name in names]\n",
    "    df_final = pd.concat(df_list,axis=0)\n",
    "    #df = pd.read_csv('news_feeds.csv')\n",
    "    #df_final = pd.concat([df,df_final],axis=0)\n",
    "    df_final.to_csv(dataset)\n",
    "    return df_final\n",
    "\n",
    "def arr_match(arr1,arr2):\n",
    "    if False in np.array(arr1==arr2):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def create_time_line(df,name):\n",
    "    print(name)\n",
    "    try:\n",
    "        vectors = list(df[df['HNWI']==name]['vector'])\n",
    "        clustering = AffinityPropagation(damping = 0.9).fit(vectors)\n",
    "        tim_df = df[df['HNWI']==name]\n",
    "        tim_df['Event_Id'] = clustering.labels_\n",
    "        tim_df['Event_center'] = tim_df.apply(lambda row: clustering.cluster_centers_[row['Event_Id']],axis=1)\n",
    "        tim_df['Event_Text'] = tim_df.apply(lambda row:row['Title'] if arr_match(row['vector'],row['Event_center']) else 0, axis=1)\n",
    "        tim_df['PubDate'] = pd.to_datetime(tim_df['PubDate'])\n",
    "        tim_df = tim_df.sort_values('PubDate')\n",
    "        return tim_df\n",
    "    except:\n",
    "        return df\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_daily_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Event_Text']!=0][['Event_Text', 'PubDate','HNWI']].to_csv('out/time_line.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
